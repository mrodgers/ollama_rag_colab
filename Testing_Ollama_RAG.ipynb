{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/ollama_rag_colab/blob/main/Testing_Ollama_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Chatting 'Privately' in Google Colab with Ollama/llama3.1/RAG!\n",
        "\n",
        "This notebook is designed to help you set up and run a Retrieval-Augmented Generation (RAG) system using Ollama's Llama3.1 model. Whether you're new to machine learning or an experienced developer, this notebook will guide you through the process of installing necessary packages, setting up an interactive terminal, and running a server to process and query documents.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "1. **Installing Required Packages**: Learn how to install the necessary Python packages to get started with Ollama and RAG.\n",
        "2. **Setting Up xterm**: Understand how to set up an xterm terminal within Google Colab to run shell commands.\n",
        "3. **Running Ollama Server**: Get the Ollama server up and running to serve the Llama3.1 model.\n",
        "4. **Loading and Processing Documents**: Learn how to load text and PDF files, process them, and create a vector store for efficient querying.\n",
        "5. **Building a Gradio Interface**: Create a user-friendly interface to upload files and ask questions about their content.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Open the Notebook**: Click the \"Open in Colab\" badge below to open this notebook in Google Colab.\n",
        "2. **Follow the Steps**: Execute the cells in the notebook one by one. Each cell contains code and instructions to guide you through the setup process.\n",
        "3. **Upload and Query Documents**: Use the Gradio interface to upload your documents and ask questions. The system will retrieve relevant information from the documents and provide answers.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mrodgers/ollama_rag_colab/blob/main/Testing_Ollama_RAG.ipynb)\n",
        "\n",
        "## Is Colab Safe for Private Data?\n",
        "\n",
        "NOPE. While Google Colab is generally safe for personal use, it is not recommended for handling sensitive or confidential data. Your private Colab notebooks are as secure as your private Google Docs, but always exercise caution and avoid sharing any national secrets or highly sensitive information.\n"
      ],
      "metadata": {
        "id": "yTiYVmrM1BUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After running the cell below, an xterm terminal will open within the Colab interface.\n",
        "# In the xterm terminal, you need to run a few commands to set up and start the Ollama server.\n",
        "# These commands will:\n",
        "# 1. Download and install the Ollama software.\n",
        "# 2. Pull the necessary models (llama3.1 and nomic-embed-text) from the Ollama repository.\n",
        "# 3. Start the Ollama server to serve the models.\n",
        "\n",
        "# Be patient with the below items, they are downloading a lot of data and take a few minutes.\n",
        "\n",
        "# After starting the xterm terminal, run the steps 2 and 3 in cells below the xterm, then the following command in the xterm terminal:\n",
        "# while true; do echo \"Keeping xterm and thus Ollama alive...\"; sleep 60; done &\n",
        "\n",
        "# After that, in the xterm window. Copy and paste the following commands into the xterm terminal after it opens:\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve\n",
        "# ollama pull llama3.1 &\n",
        "# ollama pull nomic-embed-text &"
      ],
      "metadata": {
        "id": "tIPk5CJ7UoZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a place for Ollama to run inside of an xterm\n",
        "!pip install colab-xterm -q\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "OTrwBMZdPL4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to set up xterm for success: Keep-Alive Script\n",
        "import time\n",
        "import requests\n",
        "\n",
        "def keep_alive():\n",
        "    while True:\n",
        "        try:\n",
        "            requests.get(\"https://www.google.com\")\n",
        "            time.sleep(60)  # Send a request every 60 seconds\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            break\n",
        "\n",
        "import threading\n",
        "thread = threading.Thread(target=keep_alive)\n",
        "thread.start()"
      ],
      "metadata": {
        "id": "UreTB8hh9a4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then run this cell, after the above completed: JavaScript Keep-Alive Snippet\n",
        "from IPython.display import Javascript\n",
        "\n",
        "keep_alive_js = \"\"\"\n",
        "function ClickConnect(){\n",
        "    console.log(\"Clicking to keep Colab awake...\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "}\n",
        "setInterval(ClickConnect, 60000);  // Click every 60 seconds\n",
        "\"\"\"\n",
        "\n",
        "display(Javascript(keep_alive_js))\n"
      ],
      "metadata": {
        "id": "lsy6FKM19jWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll jump back here after running the xterm and starting Ollama. This is where the chatbot app gets created."
      ],
      "metadata": {
        "id": "UHsDjPVUBmDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain-core langchain-community ollama beautifulsoup4 chromadb gradio pypdf"
      ],
      "metadata": {
        "id": "OSt2UgmfQAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Is Colab Safe for private data?\n",
        "\n",
        "Colab is not safe for Cisco data.\n",
        "\n",
        "In general though, depending on personal risk thresholds, it's generally safe, at least as safe as your private Google Doc is.\n",
        "\n",
        "No one can access your own private Colab notebooks. And Google has the incentive to make it as safe as possible for their reputation. But don't share any national secrets. But trust shouldn't just be handed out either.\n"
      ],
      "metadata": {
        "id": "1Vv9J7qjvkf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries and modules\n",
        "import os  # Provides functions for interacting with the operating system\n",
        "import gradio as gr  # Gradio is used to create a web-based interface\n",
        "import ollama  # Ollama is used for interacting with the Llama3.1 model\n",
        "from langchain_community.llms.ollama import Ollama  # Import the Ollama class for the Llama3.1 model\n",
        "from langchain_community.embeddings import OllamaEmbeddings  # Import OllamaEmbeddings for creating embeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader  # Import PyPDFLoader for loading PDF files\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Import text splitter for splitting documents into chunks\n",
        "from langchain_community.vectorstores import Chroma  # Import Chroma for creating a vector store\n",
        "\n",
        "# Define the model to be used\n",
        "MODEL = \"llama3.1\"\n",
        "llm = Ollama(model=MODEL)  # Initialize the Ollama model with Llama3.1\n",
        "\n",
        "# Define a simple Document class to wrap the content\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content  # The content of the document\n",
        "        self.metadata = metadata if metadata is not None else {}  # Metadata associated with the document\n",
        "\n",
        "# Function to load data from an uploaded file\n",
        "def load_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()  # Get the file extension\n",
        "\n",
        "    if file_extension == '.txt' or file_extension == '.md':  # If the file is a text or markdown file\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()  # Read the content of the file\n",
        "        except UnicodeDecodeError:\n",
        "            with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                content = f.read()  # Read the content with a different encoding if there's an error\n",
        "    elif file_extension == '.pdf':  # If the file is a PDF\n",
        "        loader = PyPDFLoader(file_path)  # Initialize the PDF loader\n",
        "        docs = loader.load()  # Load the PDF content\n",
        "        content = \"\\n\".join([doc.page_content for doc in docs])  # Join the content of all pages\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type: {}\".format(file_extension))  # Raise an error for unsupported file types\n",
        "\n",
        "    return content  # Return the content of the file\n",
        "\n",
        "# Function to process the uploaded file and create a vector store\n",
        "def process_file(file_path):\n",
        "    content = load_file(file_path)  # Load the file content\n",
        "    docs = [Document(page_content=content)]  # Wrap the content in a Document object\n",
        "\n",
        "    # Split the loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    splits = text_splitter.split_documents(docs)  # Split the document into smaller chunks\n",
        "\n",
        "    # Create Ollama embeddings and vector store\n",
        "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")  # Initialize the embeddings model\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)  # Create a vector store from the document chunks\n",
        "\n",
        "    return vectorstore  # Return the vector store\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"  # Format the prompt with the question and context\n",
        "    response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])  # Get the response from the model\n",
        "    return response['message']['content']  # Return the content of the response\n",
        "\n",
        "# Define the RAG chain itself\n",
        "def rag_chain(question, vectorstore):\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"mmr\",\n",
        "        search_kwargs={\n",
        "            'k': 5,\n",
        "            'fetch_k': 50\n",
        "            })  # Get a retriever from the vector store\n",
        "    retrieved_docs = retriever.invoke(question)  # Retrieve relevant documents based on the question\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)  # Format the context from the retrieved documents\n",
        "    return ollama_llm(question, formatted_context)  # Get the answer from the model using the formatted context\n",
        "\n",
        "# Set up and run the LLM chain (see langchain for details)\n",
        "def get_important_facts(file, question):\n",
        "    vectorstore = process_file(file.name)  # Process the uploaded file to create a vector store\n",
        "    return rag_chain(question, vectorstore)  # Get the answer to the question using the RAG setup\n",
        "\n",
        "# Improved Gradio app interface\n",
        "iface = gr.Interface(\n",
        "    fn=get_important_facts,  # The function to be called when the interface is used\n",
        "    inputs=[\n",
        "        gr.File(type=\"filepath\", file_count=\"single\", label=\"üìÅ Upload a file\"),  # Add an icon to the file upload label\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter your question here...\", label=\"‚ùì Your Question\")  # Add an icon to the textbox label\n",
        "    ],  # Define the inputs\n",
        "    outputs=gr.Textbox(label=\"üí¨ Answer\"),  # Define the output with  an icon\n",
        "    title=\"'Private' Chat using Ollama/Llama3.1\",  # Simplified title\n",
        "    description=\"Upload a file (pdf or txt) and ask questions about the provided context. The system will retrieve relevant information and provide an answer.\",  # Enhanced description\n",
        "    theme=\"default\",  # Use the default theme for a clean look\n",
        "    allow_flagging=\"never\",  # Disable flagging\n",
        "    css=\"\"\"\n",
        "        .gradio-container {\n",
        "            font-family: 'Roboto', sans-serif;  # Use a clean and modern font\n",
        "            padding: 20px;\n",
        "        }\n",
        "        .gradio-title {\n",
        "            font-size: 24px;\n",
        "            font-weight: 500;\n",
        "            color: #3f51b5;  # Use a primary color for the title\n",
        "        }\n",
        "        .gradio-description {\n",
        "            font-size: 16px;\n",
        "            color: #757575;  # Use a secondary color for the description\n",
        "        }\n",
        "        .gradio-inputs, .gradio-outputs {\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "        .gradio-inputs .gradio-file, .gradio-inputs .gradio-textbox {\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "        .gradio-button {\n",
        "            background-color: #3f51b5;  # Use a primary color for the button\n",
        "            color: white;\n",
        "            border: none;\n",
        "            padding: 10px 20px;\n",
        "            font-size: 16px;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "        .gradio-button:hover {\n",
        "            background-color: #303f9f;  # Darken the button color on hover\n",
        "        }\n",
        "    \"\"\"  # Custom CSS for styling\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)  # Launch the user interface with debug mode enabled (just in case)\n"
      ],
      "metadata": {
        "id": "FQ6C7EmFcSQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El Fin!\n",
        "Thanks to https://medium.com/@tharindumadhusanka99/llama3-rag-on-google-colab-73c43aa53281 for some of this code!"
      ],
      "metadata": {
        "id": "Tj6500d3Q-Ld"
      }
    }
  ]
}