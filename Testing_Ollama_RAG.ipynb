{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/ollama_rag_colab/blob/main/Testing_Ollama_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Chatting 'Privately' in Google Colab with Ollama/llama3.1/RAG!\n",
        "\n",
        "This notebook is designed to help you set up and run a Retrieval-Augmented Generation (RAG) system using Ollama's Llama3.1 model. Whether you're new to machine learning or an experienced developer, this notebook will guide you through the process of installing necessary packages, setting up an interactive terminal, and running a server to process and query documents.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "1. **Installing Required Packages**: Learn how to install the necessary Python packages to get started with Ollama and RAG.\n",
        "2. **Setting Up xterm**: Understand how to set up an xterm terminal within Google Colab to run shell commands.\n",
        "3. **Running Ollama Server**: Get the Ollama server up and running to serve the Llama3.1 model.\n",
        "4. **Loading and Processing Documents**: Learn how to load text and PDF files, process them, and create a vector store for efficient querying.\n",
        "5. **Building a Gradio Interface**: Create a user-friendly interface to upload files and ask questions about their content.\n",
        "\n",
        "## How to Use This Notebook\n",
        "\n",
        "1. **Open the Notebook**: Click the \"Open in Colab\" badge below to open this notebook in Google Colab.\n",
        "2. **Follow the Steps**: Execute the cells in the notebook one by one. Each cell contains code and instructions to guide you through the setup process.\n",
        "3. **Upload and Query Documents**: Use the Gradio interface to upload your documents and ask questions. The system will retrieve relevant information from the documents and provide answers.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mrodgers/ollama_rag_colab/blob/main/Testing_Ollama_RAG.ipynb)\n",
        "\n",
        "## Is Colab Safe for Private Data?\n",
        "\n",
        "While Google Colab is generally safe for personal use, it is not recommended for handling sensitive or confidential data. Your private Colab notebooks are as secure as your private Google Docs, but always exercise caution and avoid sharing any national secrets or highly sensitive information.\n"
      ],
      "metadata": {
        "id": "yTiYVmrM1BUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input the below into the xterm terminal after xterm comes up (/content# )\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama pull llama3.1 &\n",
        "# ollama pull nomic-embed-text &\n",
        "# ollama serve &\n"
      ],
      "metadata": {
        "id": "tIPk5CJ7UoZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a place for Ollama to run inside of an xterm\n",
        "!pip install colab-xterm -q\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "OTrwBMZdPL4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain-core langchain-community ollama beautifulsoup4 chromadb gradio pypdf"
      ],
      "metadata": {
        "id": "OSt2UgmfQAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Is Colab Safe for private data?\n",
        "\n",
        "Colab is not safe for Cisco data.\n",
        "\n",
        "In general though, depending on personal risk thresholds, it's generally safe, at least as safe as your private Google Doc is.\n",
        "\n",
        "No one can access your own private Colab notebooks. And Google has the incentive to make it as safe as possible for their reputation. But don't share any national secrets. But trust shouldn't just be handed out either.\n"
      ],
      "metadata": {
        "id": "1Vv9J7qjvkf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "from langchain_community.llms.ollama import Ollama\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "MODEL = \"llama3.1\"\n",
        "llm = Ollama(model=MODEL)\n",
        "\n",
        "# Define a simple Document class to wrap the content\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "# Function to load data from an uploaded file\n",
        "def load_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                content = f.read()\n",
        "    elif file_extension == '.pdf':\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        content = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type: {}\".format(file_extension))\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to process the uploaded file and create a vector store\n",
        "def process_file(file_path):\n",
        "    content = load_file(file_path)\n",
        "    docs = [Document(page_content=content)]\n",
        "\n",
        "    # Split the loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create Ollama embeddings and vector store\n",
        "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "def rag_chain(question, vectorstore):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    return ollama_llm(question, formatted_context)\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(file, question):\n",
        "    vectorstore = process_file(file.name)\n",
        "    return rag_chain(question, vectorstore)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=[gr.File(type=\"filepath\", file_count=\"single\", label=\"Upload a file\"), gr.Textbox(lines=2, placeholder=\"Enter your question here...\")],\n",
        "  outputs=\"text\",\n",
        "  title=\"My own private (mostly) chat using RAG with Ollama/Llama3.1\",\n",
        "  description=\"Upload a file and ask questions about the provided context\",\n",
        "  allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "FQ6C7EmFcSQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El Fin!\n",
        "Thanks to https://medium.com/@tharindumadhusanka99/llama3-rag-on-google-colab-73c43aa53281 for some of this code!"
      ],
      "metadata": {
        "id": "Tj6500d3Q-Ld"
      }
    }
  ]
}