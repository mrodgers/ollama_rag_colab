{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaHdfhanHOALwwbyZGn1WG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/ollama_rag_colab/blob/main/Testing_Ollama_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# type these codes in below terminal after run the cell (%xterm)\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve &\n",
        "# ollama pull llama3.1 &\n",
        "# ollama pull nomic-embed-text &"
      ],
      "metadata": {
        "id": "tIPk5CJ7UoZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "OTrwBMZdPL4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain-core langchain-community ollama beautifulsoup4 chromadb gradio"
      ],
      "metadata": {
        "id": "OSt2UgmfQAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms.ollama import Ollama"
      ],
      "metadata": {
        "id": "7Dwl3fL9TJde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"llama3.1\"\n",
        "llm = Ollama(model=MODEL)\n",
        "\n",
        "# the response here, could be used to test.\n",
        "# response = llm.invoke(\"What is the meaning of life?\")\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "G_W1kCq1QPUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "# Define a simple Document class to wrap the content\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "# Function to load data from an uploaded file\n",
        "def load_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        with open(file_path, 'r', encoding='latin-1') as f:\n",
        "            content = f.read()\n",
        "    return content\n",
        "\n",
        "# Function to process the uploaded file and create a vector store\n",
        "def process_file(file_path):\n",
        "    content = load_file(file_path)\n",
        "    docs = [Document(page_content=content)]\n",
        "\n",
        "    # Split the loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create Ollama embeddings and vector store\n",
        "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "def rag_chain(question, vectorstore):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    return ollama_llm(question, formatted_context)\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(file, question):\n",
        "    vectorstore = process_file(file.name)\n",
        "    return rag_chain(question, vectorstore)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=[gr.File(type=\"filepath\", file_count=\"single\", label=\"Upload a file\"), gr.Textbox(lines=2, placeholder=\"Enter your question here...\")],\n",
        "  outputs=\"text\",\n",
        "  title=\"RAG with Llama3.1\",\n",
        "  description=\"Upload a file and ask questions about the provided context\",\n",
        "  allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "FQ6C7EmFcSQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thanks to https://medium.com/@tharindumadhusanka99/llama3-rag-on-google-colab-73c43aa53281 for some of the code!"
      ],
      "metadata": {
        "id": "Tj6500d3Q-Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "# import ollama\n",
        "# from bs4 import BeautifulSoup as bs\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "# from langchain_community.document_loaders import WebBaseLoader\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "# # Load the data from the web URL\n",
        "# url = 'https://en.wikipedia.org/wiki/Ohiya'\n",
        "# loader = WebBaseLoader(url)\n",
        "# docs = loader.load()\n",
        "\n",
        "# # Split the loaded documents into chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "# splits = text_splitter.split_documents(docs)\n",
        "\n",
        "# # Create Ollama embeddings and vector store\n",
        "# embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "# vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "# # Define the function to call the Ollama Llama3 model\n",
        "# def ollama_llm(question, context):\n",
        "#     formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "#     response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
        "#     return response['message']['content']\n",
        "\n",
        "# # Define the RAG setup\n",
        "# retriever = vectorstore.as_retriever()\n",
        "\n",
        "# def rag_chain(question):\n",
        "#     retrieved_docs = retriever.invoke(question)\n",
        "#     formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "#     return ollama_llm(question, formatted_context)\n",
        "\n",
        "# # Define the Gradio interface\n",
        "# def get_important_facts(question):\n",
        "#     return rag_chain(question)\n",
        "\n",
        "# # Create a Gradio app interface\n",
        "# iface = gr.Interface(\n",
        "#   fn=get_important_facts,\n",
        "#   inputs=gr.Textbox(lines=2, placeholder=\"Enter your question here...\"),\n",
        "#   outputs=\"text\",\n",
        "#   title=\"RAG with Llama3.1\",\n",
        "#   description=\"Ask questions about the provided context\",\n",
        "#   allow_flagging=\"never\",\n",
        "# )\n",
        "\n",
        "# # Launch the Gradio app\n",
        "# iface.launch()\n"
      ],
      "metadata": {
        "id": "jg5gmzhVYEC_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}