{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO3wRN3e8iBo2WWSlz+6Ocd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrodgers/ollama_rag_colab/blob/main/Testing_Ollama_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input the below into the xterm terminal after xterm comes up (/content# )\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama pull llama3.1 &\n",
        "# ollama pull nomic-embed-text &\n",
        "# ollama serve &\n"
      ],
      "metadata": {
        "id": "tIPk5CJ7UoZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a place for Ollama to run inside of an xterm\n",
        "!pip install colab-xterm -q\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ],
      "metadata": {
        "id": "OTrwBMZdPL4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain langchain-core langchain-community ollama beautifulsoup4 chromadb gradio pypdf"
      ],
      "metadata": {
        "id": "OSt2UgmfQAfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Is Colab Safe for private data?\n",
        "\n",
        "Colab is not safe for Cisco data.\n",
        "\n",
        "In general though, depending on personal risk thresholds, it's generally safe, at least as safe as your private Google Doc is.\n",
        "\n",
        "No one can access your own private Colab notebooks. And Google has the incentive to make it as safe as possible for their reputation. But don't share any national secrets. But trust shouldn't just be handed out either.\n"
      ],
      "metadata": {
        "id": "1Vv9J7qjvkf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import ollama\n",
        "from langchain_community.llms.ollama import Ollama\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "\n",
        "MODEL = \"llama3.1\"\n",
        "llm = Ollama(model=MODEL)\n",
        "\n",
        "# Define a simple Document class to wrap the content\n",
        "class Document:\n",
        "    def __init__(self, page_content, metadata=None):\n",
        "        self.page_content = page_content\n",
        "        self.metadata = metadata if metadata is not None else {}\n",
        "\n",
        "# Function to load data from an uploaded file\n",
        "def load_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.txt':\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(file_path, 'r', encoding='latin-1') as f:\n",
        "                content = f.read()\n",
        "    elif file_extension == '.pdf':\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        content = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type: {}\".format(file_extension))\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to process the uploaded file and create a vector store\n",
        "def process_file(file_path):\n",
        "    content = load_file(file_path)\n",
        "    docs = [Document(page_content=content)]\n",
        "\n",
        "    # Split the loaded documents into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Create Ollama embeddings and vector store\n",
        "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "# Define the function to call the Ollama Llama3 model\n",
        "def ollama_llm(question, context):\n",
        "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
        "    response = ollama.chat(model='llama3.1', messages=[{'role': 'user', 'content': formatted_prompt}])\n",
        "    return response['message']['content']\n",
        "\n",
        "# Define the RAG setup\n",
        "def rag_chain(question, vectorstore):\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "    formatted_context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
        "    return ollama_llm(question, formatted_context)\n",
        "\n",
        "# Define the Gradio interface\n",
        "def get_important_facts(file, question):\n",
        "    vectorstore = process_file(file.name)\n",
        "    return rag_chain(question, vectorstore)\n",
        "\n",
        "# Create a Gradio app interface\n",
        "iface = gr.Interface(\n",
        "  fn=get_important_facts,\n",
        "  inputs=[gr.File(type=\"filepath\", file_count=\"single\", label=\"Upload a file\"), gr.Textbox(lines=2, placeholder=\"Enter your question here...\")],\n",
        "  outputs=\"text\",\n",
        "  title=\"My own private (mostly) chat using RAG with Ollama/Llama3.1\",\n",
        "  description=\"Upload a file and ask questions about the provided context\",\n",
        "  allow_flagging=\"never\",\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "iface.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "FQ6C7EmFcSQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El Fin!\n",
        "Thanks to https://medium.com/@tharindumadhusanka99/llama3-rag-on-google-colab-73c43aa53281 for some of this code!"
      ],
      "metadata": {
        "id": "Tj6500d3Q-Ld"
      }
    }
  ]
}